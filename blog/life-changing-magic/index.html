<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.91.2" />
<title>The Life-Changing Magic of Tidying Text | Julia Silge</title>


<meta property="twitter:site" content="@juliasilge">
<meta property="twitter:creator" content="@juliasilge">







  
    
  
<meta name="description" content="An R package for text mining using tidy data principles">


<meta property="og:site_name" content="Julia Silge">
<meta property="og:title" content="The Life-Changing Magic of Tidying Text | Julia Silge">
<meta property="og:description" content="An R package for text mining using tidy data principles" />
<meta property="og:type" content="page" />
<meta property="og:url" content="https://juliasilge.com/blog/life-changing-magic/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://juliasilge.com/img/illustrated_blog_avatar.png" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://juliasilge.com/img/illustrated_blog_avatar.png" >
    
  <meta itemprop="name" content="The Life-Changing Magic of Tidying Text">
<meta itemprop="description" content="When I went to the rOpenSci unconference about a month ago, I started work with Dave Robinson on a package for text mining using tidy data principles. What is this tidy data you keep hearing so much about? As described by Hadley Wickham, tidy data has a specific structure:
 each variable is a column each observation is a row each type of observational unit is a table  This means we end up with a data set that is in a long, skinny format instead of a wide format."><meta itemprop="datePublished" content="2016-04-29T00:00:00+00:00" />
<meta itemprop="dateModified" content="2016-04-29T00:00:00+00:00" />
<meta itemprop="wordCount" content="3302">
<meta itemprop="keywords" content="rstats," />
  
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/icon.png" type="image/x-icon">
  <link rel="icon" href="/img/icon.png" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.34750661ca065ca7ddb87b2b28cab82abf493a21c1e3852e0755fba776b031b7.css" integrity="sha256-NHUGYcoGXKfduHsrKMq4Kr9JOiHB44UuB1X7p3awMbc=" media="screen">
  
  
  <script src="/panelset.min.078a92db9bd3228df502db3d9e0453c3cf3d910abe3f8deca0ad196c7071ad41.js" type="text/javascript"></script>
  
  
  <script src="/main.min.bb67dea4a2ee41aab688effd180f2d02662e47280f0021495f2c0ce24c461f65.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://juliasilge.com/" title="Home">
      <span class="f4 fw7">Julia Silge</span>
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About me">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">The Life-Changing Magic of Tidying Text</h1>
        
        <p class="f6 measure lh-copy mv1">By Julia Silge</p>
        <p class="f7 db mv0 ttu">April 29, 2016</p>

      

      </header>
      <section class="post-body pt5 pb4">
        <p>When I went to the 
<a href="http://juliasilge.com/blog/I-Went-to-ROpenSci/" target="_blank" rel="noopener">rOpenSci unconference</a> about a month ago, I started work with 
<a href="http://varianceexplained.org/" target="_blank" rel="noopener">Dave Robinson</a> on a package for text mining using tidy data principles. What is this tidy data you keep hearing so much about? As 
<a href="https://www.jstatsoft.org/article/view/v059i10" target="_blank" rel="noopener">described by Hadley Wickham</a>, tidy data has a specific structure:</p>
<ul>
<li>each variable is a column</li>
<li>each observation is a row</li>
<li>each type of observational unit is a table</li>
</ul>
<p>This means we end up with a data set that is in a long, skinny format instead of a wide format. Tidy data sets are easier to work with, and this is no less true when one starts to work with text. Most of the tooling and infrastructure needed for text mining with tidy data frames already exists in packages like 
<a href="https://cran.r-project.org/package=dplyr" target="_blank" rel="noopener">dplyr</a>, 
<a href="https://cran.r-project.org/package=broom" target="_blank" rel="noopener">broom</a>, 
<a href="https://cran.r-project.org/package=tidyr" target="_blank" rel="noopener">tidyr</a>, and 
<a href="https://cran.r-project.org/package=ggplot2" target="_blank" rel="noopener">ggplot2</a>. Our goal in writing the tidytext package is to provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages. We got a great start on our work when we were at the unconference and recently finished getting the first release ready; this week, 
<a href="https://github.com/juliasilge/tidytext" target="_blank" rel="noopener">tidytext</a> was released 
<a href="https://cran.r-project.org/web/packages/tidytext/index.html" target="_blank" rel="noopener">on CRAN</a>!</p>




<h2 id="jane-austens-novels-can-be-so-tidy">Jane Austen&rsquo;s Novels Can Be So Tidy
  <a href="#jane-austens-novels-can-be-so-tidy"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>One of the important functions we knew we needed was something to unnest text by some chosen token. In my 
<a href="http://juliasilge.com/blog/You-Must-Allow-Me/" target="_blank" rel="noopener">previous</a> blog 
<a href="http://juliasilge.com/blog/If-I-Loved-NLP-Less/" target="_blank" rel="noopener">posts</a>, I did this with a <code>for</code> loop first and then with a function that involved several dplyr bits. In the tidytext package, there is a function <code>unnest_tokens</code> which has this functionality; it restructures text into a one-token-per-row format. This function is a way to convert a dataframe with a text column to be one-token-per-row. Let&rsquo;s look at an example using Jane Austen&rsquo;s novels.</p>
<p>(What?! Surely you&rsquo;re not <em>tired</em> of them yet?)</p>
<iframe src="https://giphy.com/embed/AmbVLKsrsbdjG" width="480" height="252" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/pride-and-prejudice-mr-darcy-AmbVLKsrsbdjG">via GIPHY</a></p>
<p>The janeaustenr package has a function <code>austen_books</code> that returns a tidy dataframe of all of the novels. Let&rsquo;s use that, annotate a <code>linenumber</code> quantity to keep track of lines in the original format, use a regex to find where all the chapters are, and then <code>unnest_tokens</code>.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(janeaustenr)
library(tidytext)
library(dplyr)
library(stringr)

original_books &lt;- austen_books() %&gt;%
        group_by(book) %&gt;%
        mutate(linenumber = row_number(),
               chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\divxlc]&quot;,
                                                 ignore_case = TRUE)))) %&gt;%
        ungroup()

original_books
</code></pre><pre tabindex="0"><code>## Source: local data frame [70,942 x 4]
## 
##                     text                book linenumber chapter
##                    (chr)              (fctr)      (int)   (int)
## 1  SENSE AND SENSIBILITY Sense &amp; Sensibility          1       0
## 2                        Sense &amp; Sensibility          2       0
## 3         by Jane Austen Sense &amp; Sensibility          3       0
## 4                        Sense &amp; Sensibility          4       0
## 5                 (1811) Sense &amp; Sensibility          5       0
## 6                        Sense &amp; Sensibility          6       0
## 7                        Sense &amp; Sensibility          7       0
## 8                        Sense &amp; Sensibility          8       0
## 9                        Sense &amp; Sensibility          9       0
## 10             CHAPTER 1 Sense &amp; Sensibility         10       1
## ..                   ...                 ...        ...     ...
</code></pre><pre tabindex="0"><code class="language-{r}" data-lang="{r}">tidy_books &lt;- original_books %&gt;%
        unnest_tokens(word, text)

tidy_books
</code></pre><pre tabindex="0"><code>## Source: local data frame [724,971 x 4]
## 
##                   book linenumber chapter        word
##                 (fctr)      (int)   (int)       (chr)
## 1  Sense &amp; Sensibility          1       0       sense
## 2  Sense &amp; Sensibility          1       0         and
## 3  Sense &amp; Sensibility          1       0 sensibility
## 4  Sense &amp; Sensibility          3       0          by
## 5  Sense &amp; Sensibility          3       0        jane
## 6  Sense &amp; Sensibility          3       0      austen
## 7  Sense &amp; Sensibility          5       0        1811
## 8  Sense &amp; Sensibility         10       1     chapter
## 9  Sense &amp; Sensibility         10       1           1
## 10 Sense &amp; Sensibility         13       1         the
## ..                 ...        ...     ...         ...
</code></pre><p>This function uses the 
<a href="https://github.com/lmullen/tokenizers" target="_blank" rel="noopener">tokenizers package</a> to separate each line into words. The default tokenizing is for words, but other options include characters, sentences, lines, paragraphs, or separation around a regex pattern.</p>
<p>Now that the data is in one-word-per-row format, the TIDY DATA MAGIC can happen and we can manipulate it with tidy tools like dplyr. For example, we can remove stop words (kept in the tidytext dataset <code>stop_words</code>) with an <code>anti_join</code>.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">data(&quot;stop_words&quot;)
tidy_books &lt;- tidy_books %&gt;%
        anti_join(stop_words)
</code></pre><p>Then we can use <code>count</code> to find the most common words in all of Jane Austen&rsquo;s novels as a whole.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tidy_books %&gt;%
        count(word, sort = TRUE)
</code></pre><pre tabindex="0"><code>## Source: local data frame [13,896 x 2]
## 
##      word     n
##     (chr) (int)
## 1    miss  1854
## 2    time  1337
## 3   fanny   862
## 4    dear   822
## 5    lady   817
## 6     sir   806
## 7     day   797
## 8    emma   787
## 9  sister   727
## 10  house   699
## ..    ...   ...
</code></pre><p>Sentiment analysis can be done as an inner join. Three sentiment lexicons are in the tidytext package in the <code>sentiment</code> dataset. Let&rsquo;s examine how sentiment changes changes during each novel. Let&rsquo;s find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(tidyr)
bing &lt;- sentiments %&gt;%
        filter(lexicon == &quot;bing&quot;) %&gt;%
        select(-score)

bing
</code></pre><pre tabindex="0"><code>## Source: local data frame [6,788 x 3]
## 
##           word sentiment lexicon
##          (chr)     (chr)   (chr)
## 1      2-faced  negative    bing
## 2      2-faces  negative    bing
## 3           a+  positive    bing
## 4     abnormal  negative    bing
## 5      abolish  negative    bing
## 6   abominable  negative    bing
## 7   abominably  negative    bing
## 8    abominate  negative    bing
## 9  abomination  negative    bing
## 10       abort  negative    bing
## ..         ...       ...     ...
</code></pre><pre tabindex="0"><code class="language-{r}" data-lang="{r}">janeaustensentiment &lt;- tidy_books %&gt;%
        inner_join(bing) %&gt;% 
        count(book, index = linenumber %/% 80, sentiment) %&gt;% 
        spread(sentiment, n, fill = 0) %&gt;% 
        mutate(sentiment = positive - negative)

janeaustensentiment
</code></pre><pre tabindex="0"><code>## Source: local data frame [891 x 5]
## Groups: book, index [891]
## 
##                   book index negative positive sentiment
##                 (fctr) (dbl)    (dbl)    (dbl)     (dbl)
## 1  Sense &amp; Sensibility     0       16       26        10
## 2  Sense &amp; Sensibility     1       19       44        25
## 3  Sense &amp; Sensibility     2       12       23        11
## 4  Sense &amp; Sensibility     3       15       22         7
## 5  Sense &amp; Sensibility     4       16       29        13
## 6  Sense &amp; Sensibility     5       16       39        23
## 7  Sense &amp; Sensibility     6       24       37        13
## 8  Sense &amp; Sensibility     7       22       39        17
## 9  Sense &amp; Sensibility     8       30       35         5
## 10 Sense &amp; Sensibility     9       14       18         4
## ..                 ...   ...      ...      ...       ...
</code></pre><p>Now we can plot these sentiment scores across the plot trajectory of each novel.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(ggplot2)
library(viridis)
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
        geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
        facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) +
        theme_minimal(base_size = 13) +
        labs(title = &quot;Sentiment in Jane Austen's Novels&quot;,
             y = &quot;Sentiment&quot;) +
        scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1) +
        scale_x_discrete(expand=c(0.02,0)) +
        theme(strip.text=element_text(hjust=0)) +
        theme(strip.text = element_text(face = &quot;italic&quot;)) +
        theme(axis.title.x=element_blank()) +
        theme(axis.ticks.x=element_blank()) +
        theme(axis.text.x=element_blank())
</code></pre><p><img src="/figs/2016-04-29-Life-Changing-Magic/unnamed-chunk-6-1.png" alt="center"></p>
<p>This is similar to some of the plots I have made in previous posts, but the effort and time required to make it is drastically less. More importantly, the <em>thinking</em> required to make it comes much more easily because it all falls so naturally out of joins and other <code>dplyr</code> verbs.</p>




<h2 id="looking-at-units-beyond-words">Looking at Units Beyond Words
  <a href="#looking-at-units-beyond-words"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Lots of useful work can be done by tokenizing at the word level, but sometimes it is useful or necessary to look at different units of text. For example, some sentiment analysis algorithms look beyond only unigrams (i.e. single words) to try to understand the sentiment of a sentence as a whole. These algorithms try to understand that</p>
<blockquote>
<p>I am not having a good day.</p>
</blockquote>
<p>is a negative sentence, not a positive one, because of negation. The 
<a href="http://stanfordnlp.github.io/CoreNLP/" target="_blank" rel="noopener">Stanford CoreNLP</a> tools and the 
<a href="https://github.com/trinker/sentimentr" target="_blank" rel="noopener">sentimentr R package</a> (currently available on Github but not CRAN) are examples of such sentiment analysis algorithms. For these, we may want to tokenize text into sentences.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">austen_sentences &lt;- austen_books() %&gt;% 
        group_by(book) %&gt;% 
        unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;% 
        ungroup()
</code></pre><p>Let&rsquo;s look at just one.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">austen_sentences$sentence[39]
</code></pre><pre tabindex="0"><code>## [1] &quot;it would be enough to make them completely easy.&quot;
</code></pre><p>The sentence tokenizing does seem to have a bit of trouble with UTF-8 encoded text, especially with sections of dialogue; it does much better with punctuation in ASCII.</p>
<p>Near the beginning of this vignette, we used a regex to find where all the chapters were in Austen&rsquo;s novels. We can use tidy text analysis to ask questions such as what are the most negative chapters in each of Jane Austen&rsquo;s novels? First, let&rsquo;s get the list of negative words from the Bing lexicon. Second, let&rsquo;s make a dataframe of how many words are in each chapter so we can normalize for the length of chapters. Then, let&rsquo;s find the number of negative words in each chapter and divide by the total words in each chapter. Which chapter has the highest proportion of negative words?</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">bingnegative &lt;- sentiments %&gt;%
        filter(lexicon == &quot;bing&quot;, sentiment == &quot;negative&quot;)

wordcounts &lt;- tidy_books %&gt;%
        group_by(book, chapter) %&gt;%
        summarize(words = n())

tidy_books %&gt;%
        semi_join(bingnegative) %&gt;%
        group_by(book, chapter) %&gt;%
        summarize(negativewords = n()) %&gt;%
        left_join(wordcounts, by = c(&quot;book&quot;, &quot;chapter&quot;)) %&gt;%
        mutate(ratio = negativewords/words) %&gt;%
        filter(chapter != 0) %&gt;%
        top_n(1)
</code></pre><pre tabindex="0"><code>## Source: local data frame [6 x 5]
## Groups: book [6]
## 
##                  book chapter negativewords words     ratio
##                (fctr)   (int)         (int) (int)     (dbl)
## 1 Sense &amp; Sensibility      29           172  1135 0.1515419
## 2   Pride &amp; Prejudice      34           108   646 0.1671827
## 3      Mansfield Park      45           132   884 0.1493213
## 4                Emma      15           147  1012 0.1452569
## 5    Northanger Abbey      27            55   337 0.1632047
## 6          Persuasion      21           215  1948 0.1103696
</code></pre><p>These are the chapters with the most negative words in each book, normalized for number of words in the chapter. What is happening in these chapters? In Chapter 29 of <em>Sense and Sensibility</em> Marianne finds out what an awful jerk Willoughby is by letter, and in Chapter 34 of <em>Pride and Prejudice</em> Mr. Darcy proposes for the first time (so badly!). Chapter 45 of <em>Mansfield Park</em> is almost the end, when Tom is sick with consumption and Mary is revealed as all greedy and a gold-digger, Chapter 15 of <em>Emma</em> is when horrifying Mr. Elton proposes, and Chapter 27 of <em>Northanger Abbey</em> is a short chapter where Catherine gets a terrible letter from her inconstant friend Isabella. Chapter 21 of <em>Persuasion</em> is when Anne&rsquo;s friend tells her all about Mr. Elliott&rsquo;s immoral past.</p>
<p>Interestingly, many of those chapters are very close to the ends of the novels; things tend to get really bad for Jane Austen&rsquo;s characters before their happy endings, it seems. Also, these chapters largely involve terrible revelations about characters through letters or conversations about past events, rather than some action happening directly in the plot. All that, just with <code>dplyr</code> verbs, because the data is tidy.</p>




<h2 id="networks-of-words">Networks of Words
  <a href="#networks-of-words"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Another function in tidytext is <code>pair_count</code>, which counts pairs of items that occur together within a group. Let&rsquo;s count the words that occur together in the lines of <em>Pride and Prejudice</em>.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">pride_prejudice_words &lt;- tidy_books %&gt;%
        filter(book == &quot;Pride &amp; Prejudice&quot;)
word_cooccurences &lt;- pride_prejudice_words %&gt;%
        pair_count(linenumber, word, sort = TRUE)
word_cooccurences
</code></pre><pre tabindex="0"><code>## Source: local data frame [50,550 x 3]
## 
##       value1  value2     n
##        (chr)   (chr) (dbl)
## 1  catherine    lady    87
## 2    bingley    miss    68
## 3     bennet    miss    65
## 4      darcy    miss    46
## 5    william     sir    35
## 6     bourgh      de    32
## 7  elizabeth    miss    29
## 8  elizabeth    jane    27
## 9  elizabeth   cried    24
## 10   forster colonel    24
## ..       ...     ...   ...
</code></pre><p>This can be useful, for example, to plot a network of co-occuring words with the 
<a href="http://igraph.org/" target="_blank" rel="noopener">igraph</a> and 
<a href="https://github.com/thomasp85/ggraph" target="_blank" rel="noopener">ggraph</a> packages.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(igraph)
library(ggraph)

set.seed(1813)
word_cooccurences %&gt;%
        filter(n &gt;= 10) %&gt;%
        graph_from_data_frame() %&gt;%
        ggraph(layout = &quot;fr&quot;) +
        geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = &quot;darkslategray4&quot;, size = 5) +
        geom_node_text(aes(label = name), vjust = 1.8) +
        ggtitle(expression(paste(&quot;Word Network in Jane Austen's &quot;, 
                                 italic(&quot;Pride and Prejudice&quot;)))) +
        theme_void()
</code></pre><p><img src="/figs/2016-04-29-Life-Changing-Magic/unnamed-chunk-11-1.png" alt="center"></p>
<p>Ten/five/whatever 
<a href="http://www.jasna.org/persuasions/printed/number12/heldman.htm" target="_blank" rel="noopener">thousand pounds</a> a year!</p>
<p>Let&rsquo;s do another one!</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">pride_prejudice_words &lt;- tidy_books %&gt;%
        filter(book == &quot;Emma&quot;)
word_cooccurences &lt;- pride_prejudice_words %&gt;%
        pair_count(linenumber, word, sort = TRUE)
set.seed(2016)
word_cooccurences %&gt;%
        filter(n &gt;= 10) %&gt;%
        graph_from_data_frame() %&gt;%
        ggraph(layout = &quot;fr&quot;) +
        geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = &quot;plum4&quot;, size = 5) +
        geom_node_text(aes(label = name), vjust = 1.8) +
        ggtitle(expression(paste(&quot;Word Network in Jane Austen's &quot;, 
                                 italic(&quot;Emma&quot;)))) +
        theme_void()
</code></pre><p><img src="/figs/2016-04-29-Life-Changing-Magic/unnamed-chunk-12-1.png" alt="center"></p>
<p>Lots of proper nouns are showing up in these network plots (Box Hill, Frank Churchill, Lady Catherine de Bourgh, etc.), and it is easy to pick out the main characters (Elizabeth, Emma). This type of network analysis is mainly showing us the important people and places in a text, and how they are related.</p>




<h2 id="word-frequencies">Word Frequencies
  <a href="#word-frequencies"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>A common task in text mining is to look at word frequencies and to compare frequencies across different texts. We can do this using tidy data principles pretty smoothly. We already have Jane Austen&rsquo;s works; let&rsquo;s get two more sets of texts to compare to. Dave has just put together a new 
<a href="https://github.com/dgrtwo/gutenbergr" target="_blank" rel="noopener">package to search and download books from Project Gutenberg</a> through R; we&rsquo;re going to use that because this is a better way to follow Project Gutenberg&rsquo;s rules for robot access. And it is SO nice to use! First, let&rsquo;s look at some science fiction and fantasy novels by H.G. Wells, who lived in the late 19th and early 20th centuries. Let&rsquo;s get 
<a href="https://www.gutenberg.org/ebooks/35" target="_blank" rel="noopener"><em>The Time Machine</em></a>, 
<a href="https://www.gutenberg.org/ebooks/36" target="_blank" rel="noopener"><em>The War of the Worlds</em></a>, 
<a href="https://www.gutenberg.org/ebooks/5230" target="_blank" rel="noopener"><em>The Invisible Man</em></a>, and 
<a href="https://www.gutenberg.org/ebooks/159" target="_blank" rel="noopener"><em>The Island of Doctor Moreau</em></a>.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(gutenbergr)
hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159))
tidy_hgwells &lt;- hgwells %&gt;%
        unnest_tokens(word, text) %&gt;%
        anti_join(stop_words)
</code></pre><p>Just for kicks, what are the most common words in these novels of H.G. Wells?</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tidy_hgwells %&gt;%
        count(word, sort = TRUE)
</code></pre><pre tabindex="0"><code>## Source: local data frame [11,769 x 2]
## 
##      word     n
##     (chr) (int)
## 1    time   454
## 2  people   302
## 3    door   260
## 4   heard   249
## 5   black   232
## 6   stood   229
## 7   white   222
## 8    hand   218
## 9    kemp   213
## 10   eyes   210
## ..    ...   ...
</code></pre><p>Now let&rsquo;s get some well-known works of the Brontë sisters, whose lives overlapped with Jane Austen&rsquo;s somewhat but who wrote in a bit of a different style. Let&rsquo;s get 
<a href="https://www.gutenberg.org/ebooks/1260" target="_blank" rel="noopener"><em>Jane Eyre</em></a>, 
<a href="https://www.gutenberg.org/ebooks/768" target="_blank" rel="noopener"><em>Wuthering Heights</em></a>, 
<a href="https://www.gutenberg.org/ebooks/969" target="_blank" rel="noopener"><em>The Tenant of Wildfell Hall</em></a>, 
<a href="https://www.gutenberg.org/ebooks/9182" target="_blank" rel="noopener"><em>Villette</em></a>, and 
<a href="https://www.gutenberg.org/ebooks/767" target="_blank" rel="noopener"><em>Agnes Grey</em></a>.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 766))
tidy_bronte &lt;- bronte %&gt;%
        unnest_tokens(word, text) %&gt;%
        anti_join(stop_words)
</code></pre><p>What are the most common words in these novels of the Brontë sisters?</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tidy_bronte %&gt;%
        count(word, sort = TRUE)
</code></pre><pre tabindex="0"><code>## Source: local data frame [25,714 x 2]
## 
##      word     n
##     (chr) (int)
## 1    time  1586
## 2    miss  1388
## 3    hand  1239
## 4     day  1136
## 5    eyes  1023
## 6   night  1011
## 7   house   960
## 8    head   957
## 9  looked   949
## 10   aunt   896
## ..    ...   ...
</code></pre><p>Well, Jane Austen is not going around talking about people&rsquo;s HEARTS this much; I can tell you that right now. Those Brontë sisters, SO DRAMATIC. Interesting that &ldquo;time&rdquo; and &ldquo;door&rdquo; are in the top 10 for both H.G. Wells and the Brontë sisters. &ldquo;Door&rdquo;?!</p>
<p>Anyway, let&rsquo;s calculate the frequency for each word for the works of Jane Austen, the Brontë sisters, and H.G. Wells.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tidy_both &lt;- bind_rows(
        mutate(tidy_bronte, author = &quot;Brontë Sisters&quot;),
        mutate(tidy_hgwells, author = &quot;H.G. Wells&quot;))
frequency &lt;- tidy_both %&gt;%
        mutate(word = str_extract(word, &quot;[a-z]+&quot;)) %&gt;%
        count(author, word) %&gt;%
        rename(other = n) %&gt;%
        inner_join(count(tidy_books, word)) %&gt;%
        rename(Austen = n) %&gt;%
        mutate(other = other / sum(other),
               Austen = Austen / sum(Austen)) %&gt;%
        ungroup()
</code></pre><p>I&rsquo;m using <code>str_extract</code> here because the UTF-8 encoded texts from Project Gutenberg have some examples of words with underscores around them to indicate emphasis (you know, like italics). The tokenizer treated these as words but I don&rsquo;t want to count &ldquo;_any_&rdquo; separately from &ldquo;any&rdquo;. Now let&rsquo;s plot.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(scales)
ggplot(frequency, aes(x = other, y = Austen, color = abs(Austen - other))) +
        geom_abline(color = &quot;gray40&quot;) +
        geom_jitter(alpha = 0.1, size = 2.5, width = 0.4, height = 0.4) +
        geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
        scale_x_log10(labels = percent_format()) +
        scale_y_log10(labels = percent_format()) +
        scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) +
        facet_wrap(~author, ncol = 2) +
        theme_minimal(base_size = 14) +
        theme(legend.position=&quot;none&quot;) +
        labs(title = &quot;Comparing Word Frequencies&quot;,
             subtitle = &quot;Word frequencies in Jane Austen's texts are closer to the Brontë sisters' than to H.G. Wells'&quot;,
             y = &quot;Jane Austen&quot;, x = NULL)
</code></pre><p><img src="/figs/2016-04-29-Life-Changing-Magic/unnamed-chunk-18-1.png" alt="center"></p>
<p>Words that are close to the line in these plots have similar frequencies in both sets of texts, for example, in both Austen and Brontë texts (&ldquo;miss&rdquo;, &ldquo;time&rdquo;, &ldquo;lady&rdquo;, &ldquo;day&rdquo; at the upper frequency end) or in both Austen and Wells texts (&ldquo;time&rdquo;, &ldquo;day&rdquo;, &ldquo;mind&rdquo;, &ldquo;brother&rdquo; at the high frequency end). Words that are far from the line are words that are found more in one set of texts than another. For example, in the Austen-Brontë plot, words like &ldquo;elizabeth&rdquo;, &ldquo;emma&rdquo;, &ldquo;captain&rdquo;, and &ldquo;bath&rdquo; (all proper nouns) are found in Austen&rsquo;s texts but not much in the Brontë texts, while words like &ldquo;arthur&rdquo;, &ldquo;dark&rdquo;, &ldquo;dog&rdquo;, and &ldquo;doctor&rdquo; are found in the Brontë texts but not the Austen texts. In comparing H.G. Wells with Jane Austen, Wells uses words like &ldquo;beast&rdquo;, &ldquo;guns&rdquo;, &ldquo;brute&rdquo;, and &ldquo;animal&rdquo; that Austen does not, while Austen uses words like &ldquo;family&rdquo;, &ldquo;friend&rdquo;, &ldquo;letter&rdquo;, and &ldquo;agreeable&rdquo; that Wells does not.</p>
<p>Overall, notice that the words in the Austen-Brontë plot are closer to the zero-slope line than in the Austen-Wells plot and also extend to lower frequencies; Austen and the Brontë sisters use more similar words than Austen and H.G. Wells. Also, you might notice the percent frequencies for individual words are different in one plot when compared to another because of the inner join; not all the words are found in all three sets of texts so the percent frequency is a different quantity.</p>
<p>Let&rsquo;s quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the Brontë sisters, and between Austen and Wells?</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">cor.test(data = frequency[frequency$author == &quot;Brontë Sisters&quot;,], ~ other + Austen)
</code></pre><pre tabindex="0"><code>## 
## 	Pearson's product-moment correlation
## 
## data:  other and Austen
## t = 122.45, df = 10611, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7572399 0.7730119
## sample estimates:
##       cor 
## 0.7652408
</code></pre><pre tabindex="0"><code class="language-{r}" data-lang="{r}">cor.test(data = frequency[frequency$author == &quot;H.G. Wells&quot;,], ~ other + Austen)
</code></pre><pre tabindex="0"><code>## 
## 	Pearson's product-moment correlation
## 
## data:  other and Austen
## t = 36.043, df = 5958, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4020291 0.4437216
## sample estimates:
##       cor 
## 0.4230993
</code></pre><p>The relationship between the word frequencies is different between these sets of texts, as it appears in the plots.</p>




<h2 id="the-end">The End
  <a href="#the-end"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>There is another whole set of functions in tidytext for converting to and from document-term matrices. Many existing text mining data sets are in document-term matrices, or you might want such a matrix for a specific machine learning application. The tidytext package has <code>tidy</code> functions for objects from the tm and quanteda packages so you can convert back and forth. (For more on the <code>tidy</code> verb, see the 
<a href="https://github.com/dgrtwo/broom" target="_blank" rel="noopener">broom package</a>). This allows, for example, a workflow with easy reading, filtering, and processing to be done using dplyr and other tidy tools, after which the data can be converted into a document-term matrix for machine learning applications. For examples of working with objects from other text mining packages using tidy data principles, see the tidytext vignette on converting to and from document-term matrices.</p>
<p>Many thanks to 
<a href="https://ropensci.org/" target="_blank" rel="noopener">rOpenSci</a> for hosting the unconference where we started work on the tidytext package, and to 
<a href="http://gdequeiroz.github.io/" target="_blank" rel="noopener">Gabriela de Queiroz</a>, who contributed to the package while we were at the unconference. I am super happy to have collaborated with Dave; it has been a delightful experience. The R Markdown file used to make this blog post is available 
<a href="https://github.com/juliasilge/juliasilge.github.io/blob/master/_R/2016-04-29-Life-Changing-Magic.Rmd" target="_blank" rel="noopener">here</a>. I am very happy to hear feedback or questions!</p>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">April 29, 2016</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">16 minute read, 3302 words</dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Tags:</dt>
    <dd class="fw5 ml0"> <a href="https://juliasilge.com/tags/rstats">rstats</a> </dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
    <dd class="fw5 ml0"><a href="/blog/educational-attainment/">Educational attainment in #TidyTuesday UK towns</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/polling-places/">Changes in #TidyTuesday US polling places</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/doctor-who-bayes/">Empirical Bayes for #TidyTuesday Doctor Who episodes</a></dd>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="https://juliasilge.com/blog/beginners-guide-to-travis/">&larr; A Beginner&#39;s Guide to Travis-CI for R</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="https://juliasilge.com/blog/how-i-stopped/">How I Learned to Stop Worrying and Love R CMD Check &rarr;</a>
  
</div>

      </footer>
    </article>
    
      
<div class="post-comments pa0 pa4-l mt4">
  
  <script src="https://utteranc.es/client.js"
          repo="juliasilge/juliasilge.com"
          issue-term="title"
          theme="github-light"
          label="comments :speech_balloon:"
          crossorigin="anonymous"
          async
          type="text/javascript">
  </script>
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2024 Julia Silge
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
