<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.91.2" />
<title>Joy to the World, and also Anticipation, Disgust, Surprise... | Julia Silge</title>


<meta property="twitter:site" content="@juliasilge">
<meta property="twitter:creator" content="@juliasilge">







  
    
  
<meta name="description" content="Natural Language Processing and Sentiment Analysis of My Twitter Archive">


<meta property="og:site_name" content="Julia Silge">
<meta property="og:title" content="Joy to the World, and also Anticipation, Disgust, Surprise... | Julia Silge">
<meta property="og:description" content="Natural Language Processing and Sentiment Analysis of My Twitter Archive" />
<meta property="og:type" content="page" />
<meta property="og:url" content="https://juliasilge.com/blog/joy-to-the-world/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://juliasilge.com/blog/joy-to-the-world/featured.png" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://juliasilge.com/blog/joy-to-the-world/featured.png" >
    
    
  <meta itemprop="name" content="Joy to the World, and also Anticipation, Disgust, Surprise...">
<meta itemprop="description" content="In my previous blog post, I analyzed my Twitter archive and explored some aspects of my tweeting behavior. When do I tweet, how much do retweet people, do I use hashtags? These are examples of one kind of question, but what about the actual verbal content of my tweets, the text itself? What kinds of questions can we ask and answer about the text in some programmatic way? This is what is called natural language processing, and I&rsquo;ll give a first shot at it here."><meta itemprop="datePublished" content="2015-12-22T00:00:00+00:00" />
<meta itemprop="dateModified" content="2015-12-22T00:00:00+00:00" />
<meta itemprop="wordCount" content="2691"><meta itemprop="image" content="https://juliasilge.com/blog/joy-to-the-world/featured.png">
<meta itemprop="keywords" content="rstats," />
  
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/icon.png" type="image/x-icon">
  <link rel="icon" href="/img/icon.png" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.34750661ca065ca7ddb87b2b28cab82abf493a21c1e3852e0755fba776b031b7.css" integrity="sha256-NHUGYcoGXKfduHsrKMq4Kr9JOiHB44UuB1X7p3awMbc=" media="screen">
  
  
  <script src="/panelset.min.078a92db9bd3228df502db3d9e0453c3cf3d910abe3f8deca0ad196c7071ad41.js" type="text/javascript"></script>
  
  
  <script src="/main.min.bb67dea4a2ee41aab688effd180f2d02662e47280f0021495f2c0ce24c461f65.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://juliasilge.com/" title="Home">
      <span class="f4 fw7">Julia Silge</span>
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About me">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Joy to the World, and also Anticipation, Disgust, Surprise...</h1>
        <h4 class="f4 mt0 mb4 lh-title measure">Natural Language Processing and Sentiment Analysis of My Twitter Archive</h4>
        <p class="f6 measure lh-copy mv1">By Julia Silge in <a href="https://juliasilge.com/categories/rstats">rstats</a> </p>
        <p class="f7 db mv0 ttu">December 22, 2015</p>

      

      </header>
      <section class="post-body pt5 pb4">
        <p>In my 
<a href="http://juliasilge.com/blog/Ten-Thousand-Tweets/" target="_blank" rel="noopener">previous blog post</a>, I analyzed my Twitter archive and explored some aspects of my tweeting behavior. When do I tweet, how much do retweet people, do I use hashtags? These are examples of one kind of question, but what about the actual verbal content of my tweets, the text itself? What kinds of questions can we ask and answer about the text in some programmatic way? This is what is called natural language processing, and I&rsquo;ll give a first shot at it here. First, let&rsquo;s load some libraries I&rsquo;ll use in this analysis and open up my Twitter archive.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(tm)
library(stringr)
library(wordcloud)

tweets &lt;- read.csv(&quot;./tweets.csv&quot;, stringsAsFactors = FALSE)
</code></pre>



<h2 id="cloudy-with-a-chance-of-lots-of-words">Cloudy With a Chance of Lots of Words
  <a href="#cloudy-with-a-chance-of-lots-of-words"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Perhaps the simplest question we might want to ask is about the frequency of words used in text. What words do I use a lot in my tweets? Let&rsquo;s find out, and make a common visualization of term frequency called a word cloud.</p>
<p>First, let&rsquo;s remove the Twitter handles (i.e., <code>@juliasilge</code> or similar) from the tweet texts so we can just have the real English words.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">nohandles &lt;- str_replace_all(tweets$text, &quot;@\\w+&quot;, &quot;&quot;)
</code></pre><p>Now, let&rsquo;s go through and clean up the remaining text. We will turn this into a &ldquo;corpus&rdquo;, a collection of documents containing natural language text that the <code>tm</code> text mining package knows how to deal with. After that, we&rsquo;ll go through and apply specific mappings to the documents (i.e., tweet texts). These can actually be done all in one call to <code>tm_map</code> but I have them separated here to show clearly what is going on.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">wordCorpus &lt;- Corpus(VectorSource(nohandles))
wordCorpus &lt;- tm_map(wordCorpus, removePunctuation)
wordCorpus &lt;- tm_map(wordCorpus, content_transformer(tolower))
wordCorpus &lt;- tm_map(wordCorpus, removeWords, stopwords(&quot;english&quot;))
wordCorpus &lt;- tm_map(wordCorpus, removeWords, c(&quot;amp&quot;, &quot;2yo&quot;, &quot;3yo&quot;, &quot;4yo&quot;))
wordCorpus &lt;- tm_map(wordCorpus, stripWhitespace)
</code></pre><p>The mappings do pretty much what they sound like they do. First, we remove all punctuation, then we change everything to all lower case. The next line removes English stop words; these are very common words that aren&rsquo;t super meaningful. If we didn&rsquo;t remove stop words, then most people&rsquo;s text will look pretty much the same and my word cloud would just be an enormous <em>the</em> with various prepositions and conjunctions and such around it. In the next line, let&rsquo;s remove some &ldquo;words&rdquo; that were leftover from how special characters were treated in the Twitter archive. The last line strips whitespace from before and after each word so that now the corpus holds nice, neat, cleaned-up words.</p>
<p>A common transformation or mapping to do when examining term frequency is called stemming; this means to take words like <em>cleaning</em> and <em>cleaned</em> and <em>cleaner</em> and remove the endings of the words to keep just <em>clean</em> for all of these examples. Then we can count all of them together instead of separately, which sounds like it would be an excellent thing to do. The stemming algorithm in <code>tm</code> did not work very well for my purposes for my tweet texts, however. It turned some common words in my tweet texts into misspellings; it changed <em>baby</em> to <em>babi</em>, <em>morning</em> to <em>morn</em>, <em>little</em> to <em>littl</em>, etc.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}"># did not do this stemming
wordCorpus &lt;- tm_map(wordCorpus, stemDocument)
</code></pre><p>This might actually be a sensible, great thing to do if my ultimate goal was to algorithmically compare my corpus to another corpus. It wouldn&rsquo;t much matter if the words being counted were correctly spelled English words, and it would make more difference pay attention to stemming. However, I want to make a word cloud for us all to look at, so I don&rsquo;t want to have anything nonsensical or incorrectly spelled; I will leave out stemming. It did not make a big difference in my results here when I did and did not include stemming. That&rsquo;s actually pretty interesting; it must be related to the tense and inflection that I use in tweets.</p>
<p>So now let&rsquo;s make the word cloud. The <code>wordcloud</code> function takes either a corpus like we have here, or a vector of words and their frequencies, or even a bunch of text, and then plots a word cloud. We can tinker with the scale of the sizes of the words to be plotted, the colors used, and how many words to plot. For what it&rsquo;s worth, I could not get <code>wordcloud</code> to play nicely with <code>svglite</code>, so I am using the default PNG format from <code>knitr</code> here.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">pal &lt;- brewer.pal(9,&quot;YlGnBu&quot;)
pal &lt;- pal[-(1:4)]
set.seed(123)
wordcloud(words = wordCorpus, scale=c(5,0.1), max.words=100, random.order=FALSE, 
          rot.per=0.35, use.r.layout=FALSE, colors=pal)
</code></pre><p><img src="/figs/2015-12-22-Joy-to-the-World/unnamed-chunk-6-1.png" alt="center"></p>
<p>I just&hellip; Like, really?</p>
<p>You can see that there are a few examples where stemming would have made sense, <em>thing</em> and <em>things</em>, for example. Also, we can definitely see here that I have been a social, personal user of Twitter since I joined in 2008. These words all look conversational, personal, and informal. We would see a very different word cloud for someone who has used Twitter for professional purposes over her whole Twitter career. Another way to quantify the information shown in the word cloud is called a term-document matrix; this is a matrix of numbers (0 and 1) that keeps track of which documents in a corpus use which terms. Let&rsquo;s see this for the corpus of my tweets.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tdm &lt;- TermDocumentMatrix(wordCorpus)
tdm
</code></pre><pre tabindex="0"><code>## &lt;&lt;TermDocumentMatrix (terms: 14712, documents: 10202)&gt;&gt;
## Non-/sparse entries: 82638/150009186
## Sparsity           : 100%
## Maximal term length: 78
## Weighting          : term frequency (tf)
</code></pre><p>So this term-document matrix contains information on 14712 terms (i.e, words) and 10202 documents (i.e., tweets). We could also make a document-term matrix, which has the rows and columns the other way around. Let&rsquo;s look at part of this matrix.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">inspect(tdm[12880:12890, 270:280])
</code></pre><pre tabindex="0"><code>## &lt;&lt;TermDocumentMatrix (terms: 11, documents: 11)&gt;&gt;
## Non-/sparse entries: 1/120
## Sparsity           : 99%
## Maximal term length: 11
## Weighting          : term frequency (tf)
## 
##              Docs
## Terms         270 271 272 273 274 275 276 277 278 279 280
##   sudafed       0   0   0   0   0   0   0   0   0   0   0
##   sudden        0   0   0   0   0   0   0   0   0   0   0
##   suddenacute   0   0   0   0   0   0   0   0   0   0   0
##   suddenly      0   0   0   0   0   0   0   0   0   0   0
##   sued          0   0   0   0   0   0   0   0   0   0   0
##   suffer        0   0   0   0   0   0   0   0   0   0   0
##   suffering     0   0   0   0   0   0   0   0   0   0   0
##   sufjan        0   0   0   0   0   0   0   0   0   0   0
##   sugar         0   0   1   0   0   0   0   0   0   0   0
##   sugarcrash    0   0   0   0   0   0   0   0   0   0   0
##   sugarhouse    0   0   0   0   0   0   0   0   0   0   0
</code></pre><p>This is how the term-document matrix keeps track of which documents (tweets, in this example) contains which terms. Notice that document 272 contains the word sugar and so has a 1 in that row/column. (And also Sufjan shows up in my term-document matrix! So seasonally appropriate. 
<a href="https://youtu.be/OYQFeZFLyM4" target="_blank" rel="noopener">Best Christmas album ever</a>? Or 
<a href="https://youtu.be/TW1AgX-Fv3M" target="_blank" rel="noopener">absolutely Christmas album ever</a>?) We can see we might want to use stemming; do we really want to keep <em>suffer</em> and <em>suffering</em> separate in this matrix? That would depend on what we want to do with it. We might want to calculate the similarity of this corpus as compared to another corpus and use the term-document matrices, or we might want to do some linear algebra like a principal component analysis on the matrix. Whatever step we might want to take next, notice what a sparse matrix this is &ndash; LOTS of zeroes and only a very few ones. This can be a good thing because we could use special numerical techniques for doing linear algebra with sparse matrices that are faster and more efficient.</p>




<h2 id="my-twitter-bffs">My Twitter BFFs
  <a href="#my-twitter-bffs"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>We made the first word cloud by getting the English words from the tweet texts, after removing the Twitter handles from them, but now let&rsquo;s look at a word cloud of just the Twitter handles from my tweet texts, my Twitter friends and acquaintances, as it were.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">friends &lt;- str_extract_all(tweets$text, &quot;@\\w+&quot;)
namesCorpus &lt;- Corpus(VectorSource(friends))
</code></pre><p>Now we have a new corpus made up of all the twitter handles extracted from all the tweet texts. These have been extracted directly from the text of the tweets, so these will include everyone who I retweet and reply to, including both more famous-type people and also personal friends.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">set.seed(146)
wordcloud(words = namesCorpus, scale=c(3,0.5), max.words=40, random.order=FALSE, 
          rot.per=0.10, use.r.layout=FALSE, colors=pal)
</code></pre><p><img src="/figs/2015-12-22-Joy-to-the-World/unnamed-chunk-10-1.png" alt="center"></p>
<p>As I look at this word cloud, I also see the evidence that I have used Twitter in a social, personal way; these are almost all personal friends, not professional contacts or the like.</p>




<h2 id="all-the-feels">All the Feels
  <a href="#all-the-feels"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>We can use an algorithm to evaluate and categorize the feelings expressed in text; this is called sentiment analysis and it can be, as you might imagine, tricky. For example, sentiment analysis algorithms are built in such a way that they are 
<a href="http://www.meetup.com/Data-Science-DC/events/226358392/" target="_blank" rel="noopener">more sensitive to expressions typical of men than women</a>. And how good will computers 
<a href="https://gate.ac.uk/sale/lrec2014/arcomem/sarcasm.pdf" target="_blank" rel="noopener">ever be at identifying sarcasm</a>? Most of these concerns won&rsquo;t have a big effect on my analysis here because I am looking at text produced by only one person (me) but it is always a good idea to keep in mind the limitations and problems in the tools we are using. Let&rsquo;s load some libraries and get sentimental.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr )
mySentiment &lt;- get_nrc_sentiment(tweets$text)
</code></pre><p>The sentiment analysis algorithm used here is based on the 
<a href="http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm" target="_blank" rel="noopener">NRC Word-Emotion Association Lexicon</a> of Saif Mohammad and Peter Turney. The idea here is that these researchers have built a dictionary/lexicon containing lots of words with associated scores for eight different emotions and two sentiments (positive/negative). Each individual word in the lexicon will have a &ldquo;yes&rdquo; (one) or &ldquo;no&rdquo; (zero) for the emotions and sentiments, and we can calculate the total sentiment of a sentence by adding up the individual sentiments for each word in the sentence. Not every English word is in the lexicon because many English words are pretty neutral.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">get_nrc_sentiment(&quot;O holy night, the stars are brightly shining&quot;)
</code></pre><pre tabindex="0"><code>##   anger anticipation disgust fear joy sadness surprise trust negative positive
## 1     0            1       0    0   1       0        0     0        0        2
</code></pre><p>All the words in that sentence have entries that are all zeroes except for &ldquo;holy&rdquo; and &ldquo;shining&rdquo;; add up the sentiment scores for those two words and you get the sentiment for the sentence.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">get_nrc_sentiment(&quot;Oh, the weather outside is frightful&quot;)
</code></pre><pre tabindex="0"><code>##   anger anticipation disgust fear joy sadness surprise trust negative positive
## 1     1            0       0    1   0       1        0     0        1        0
</code></pre><p>The only word in that sentence with any non-zero sentiment score is &ldquo;frightful&rdquo;, so the sentiment score for the whole sentence is the same as the sentiment score for that one word.</p>
<p>Let&rsquo;s look at a few of the sentiment scores for my tweets and then bind them to the data frame containing all the rest of the tweet information.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">head(mySentiment)
</code></pre><pre tabindex="0"><code>##   anger anticipation disgust fear joy sadness surprise trust negative positive
## 1     0            1       0    0   1       0        0     1        0        1
## 2     0            0       0    0   0       0        0     0        0        0
## 3     0            0       0    0   0       0        0     0        0        0
## 4     0            1       1    0   1       1        0     2        1        2
## 5     0            1       0    0   1       0        1     2        0        2
## 6     0            0       0    0   0       0        0     0        0        0
</code></pre><pre tabindex="0"><code class="language-{r}" data-lang="{r}">tweets &lt;- cbind(tweets, mySentiment)
</code></pre><p>What would some examples look like?</p>
<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">Celebrating Rob&#39;s birthday today, and about to try to get a flourless chocolate cake out of a springform pan. Wish me luck.</p>&mdash; Julia Silge (@juliasilge) <a href="https://twitter.com/juliasilge/status/546811325993586688">December 21, 2014</a></blockquote>
<script async src="http://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This tweet has anticipation and joy scores of 4, a surprise score of 2, a trust score of 1, and zero for all the other sentiments.</p>
<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">Stomach bug thoughts: cholera would be a terrible way to die.</p>&mdash; Julia Silge (@juliasilge) <a href="https://twitter.com/juliasilge/status/286200639174868992">January 1, 2013</a></blockquote>
<script async src="http://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This tweet, in contrast, has disgust and fear scores of 4, a sadness score of 3, an anger score of 1, and zeroes for all the other sentiments.</p>
<p>Let&rsquo;s look at the sentiment scores for the eight emotions from the NRC lexicon in aggregate for all my tweets. What are the most common emotions in my tweets, as measured by this sentiment analysis algorithm?</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">sentimentTotals &lt;- data.frame(colSums(tweets[,c(11:18)]))
names(sentimentTotals) &lt;- &quot;count&quot;
sentimentTotals &lt;- cbind(&quot;sentiment&quot; = rownames(sentimentTotals), sentimentTotals)
rownames(sentimentTotals) &lt;- NULL
ggplot(data = sentimentTotals, aes(x = sentiment, y = count)) +
        geom_bar(aes(fill = sentiment), stat = &quot;identity&quot;) +
        theme(legend.position = &quot;none&quot;) +
        xlab(&quot;Sentiment&quot;) + ylab(&quot;Total Count&quot;) + ggtitle(&quot;Total Sentiment Score for All Tweets&quot;)
</code></pre><p><img src="/figs/2015-12-22-Joy-to-the-World/unnamed-chunk-15-1.png" alt="center"></p>
<p>I am a positive person on Twitter, apparently, with joy, anticipation, and trust far outweighing the other emotions. Now let&rsquo;s see if the sentiment of my tweets has changed over time. We can use <code>dplyr</code> and <code>cut</code> to group the tweets into time intervals for analysis.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tweets$timestamp &lt;- with_tz(ymd_hms(tweets$timestamp), &quot;America/Chicago&quot;)
posnegtime &lt;- tweets %&gt;% 
        group_by(timestamp = cut(timestamp, breaks=&quot;2 months&quot;)) %&gt;%
        summarise(negative = mean(negative),
                  positive = mean(positive)) %&gt;% melt
names(posnegtime) &lt;- c(&quot;timestamp&quot;, &quot;sentiment&quot;, &quot;meanvalue&quot;)
posnegtime$sentiment = factor(posnegtime$sentiment,levels(posnegtime$sentiment)[c(2,1)])

ggplot(data = posnegtime, aes(x = as.Date(timestamp), y = meanvalue, group = sentiment)) +
        geom_line(size = 2.5, alpha = 0.7, aes(color = sentiment)) +
        geom_point(size = 0.5) +
        ylim(0, NA) + 
        scale_colour_manual(values = c(&quot;springgreen4&quot;, &quot;firebrick3&quot;)) +
        theme(legend.title=element_blank(), axis.title.x = element_blank()) +
        scale_x_date(breaks = date_breaks(&quot;9 months&quot;), 
                     labels = date_format(&quot;%Y-%b&quot;)) +
        ylab(&quot;Average sentiment score&quot;) + 
        ggtitle(&quot;Sentiment Over Time&quot;)
</code></pre><p><img src="/figs/2015-12-22-Joy-to-the-World/unnamed-chunk-16-1.png" alt="center"></p>
<p>The positive sentiment scores are always much higher than the negative sentiment scores, but both show some decrease over time. The positive sentiment exhibits a stronger downward trend. Apparently, I am gradually becoming less exuberant in my vocabulary on Twitter?</p>
<p>Now let&rsquo;s see if there are any associations between day of the week and the emotions from the NRC lexicon. Use the <code>wday()</code> function from the <code>lubridate</code> package and <code>dplyr</code> to group the tweets by weekday.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tweets$weekday &lt;- wday(tweets$timestamp, label = TRUE)
weeklysentiment &lt;- tweets %&gt;% group_by(weekday) %&gt;% 
        summarise(anger = mean(anger), 
                  anticipation = mean(anticipation), 
                  disgust = mean(disgust), 
                  fear = mean(fear), 
                  joy = mean(joy), 
                  sadness = mean(sadness), 
                  surprise = mean(surprise), 
                  trust = mean(trust)) %&gt;% melt
names(weeklysentiment) &lt;- c(&quot;weekday&quot;, &quot;sentiment&quot;, &quot;meanvalue&quot;)

ggplot(data = weeklysentiment, aes(x = weekday, y = meanvalue, group = sentiment)) +
        geom_line(size = 2.5, alpha = 0.7, aes(color = sentiment)) +
        geom_point(size = 0.5) +
        ylim(0, 0.6) +
        theme(legend.title=element_blank(), axis.title.x = element_blank()) +
        ylab(&quot;Average sentiment score&quot;) + 
        ggtitle(&quot;Sentiment During the Week&quot;)
</code></pre><p><img src="/figs/2015-12-22-Joy-to-the-World/unnamed-chunk-17-1.png" alt="center"></p>
<p>These are mostly consistent with being flat, although it looks like joy and anticipation may be higher on the weekends than on weekdays.</p>
<p>Similarly, let&rsquo;s see if there are associations between the eight emotions and the months of the year.</p>
<pre tabindex="0"><code class="language-{r}" data-lang="{r}">tweets$month &lt;- month(tweets$timestamp, label = TRUE)
monthlysentiment &lt;- tweets %&gt;% group_by(month) %&gt;% 
        summarise(anger = mean(anger), 
                  anticipation = mean(anticipation), 
                  disgust = mean(disgust), 
                  fear = mean(fear), 
                  joy = mean(joy), 
                  sadness = mean(sadness), 
                  surprise = mean(surprise), 
                  trust = mean(trust)) %&gt;% melt
names(monthlysentiment) &lt;- c(&quot;month&quot;, &quot;sentiment&quot;, &quot;meanvalue&quot;)

ggplot(data = monthlysentiment, aes(x = month, y = meanvalue, group = sentiment)) +
        geom_line(size = 2.5, alpha = 0.7, aes(color = sentiment)) +
        geom_point(size = 0.5) +
        ylim(0, NA) +
        theme(legend.title=element_blank(), axis.title.x = element_blank()) +
        ylab(&quot;Average sentiment score&quot;) + 
        ggtitle(&quot;Sentiment During the Year&quot;)
</code></pre><p><img src="/figs/2015-12-22-Joy-to-the-World/unnamed-chunk-18-1.png" alt="center"></p>
<p>The first thing to notice here is that the variance is much higher for the months of the year than it was for the days of the week. Most (all?) of this must be from our old friend, the Central Limit Theorem. The sample size used to calculate each point is about 12/7 bigger for the plot showing the days of the week compared to the plot showing the months of the year, so we would expect the variance to be 12/7 (or 1.714) times larger in the months-of-the-year plot. Another thing I notice is that joy and anticipation show a distinct increase in December. May your December also be full of joy and anticipation!</p>




<h2 id="the-end">The End
  <a href="#the-end"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>The R Markdown file used to make this blog post is available 
<a href="https://github.com/juliasilge/juliasilge.github.io/blob/master/_R/2015-12-22-Joy-to-the-World.Rmd" target="_blank" rel="noopener">here</a>. I am happy to hear feedback and suggestions; these are some very powerful packages I used in this analysis and I know I only scratched the surface of the kind of work that can be done with them.</p>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">December 22, 2015</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">13 minute read, 2691 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="https://juliasilge.com/categories/rstats">rstats</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Tags:</dt>
    <dd class="fw5 ml0"> <a href="https://juliasilge.com/tags/rstats">rstats</a> </dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
    <dd class="fw5 ml0"><a href="/blog/educational-attainment/">Educational attainment in #TidyTuesday UK towns</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/polling-places/">Changes in #TidyTuesday US polling places</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/doctor-who-bayes/">Empirical Bayes for #TidyTuesday Doctor Who episodes</a></dd>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="https://juliasilge.com/blog/this-is-the-place/">&larr; This Is the Place, Apparently</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="https://juliasilge.com/blog/ten-thousand-tweets/">Ten Thousand Tweets &rarr;</a>
  
</div>

      </footer>
    </article>
    
      
<div class="post-comments pa0 pa4-l mt4">
  
  <script src="https://utteranc.es/client.js"
          repo="juliasilge/juliasilge.com"
          issue-term="title"
          theme="github-light"
          label="comments :speech_balloon:"
          crossorigin="anonymous"
          async
          type="text/javascript">
  </script>
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2024 Julia Silge
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Ap√©ro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
