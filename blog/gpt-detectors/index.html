<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.91.2" />
<title>Classification metrics for #TidyTuesday GPT detectors | Julia Silge</title>


<meta property="twitter:site" content="@juliasilge">
<meta property="twitter:creator" content="@juliasilge">







  
    
  
<meta name="description" content="A data science blog">


<meta property="og:site_name" content="Julia Silge">
<meta property="og:title" content="Classification metrics for #TidyTuesday GPT detectors | Julia Silge">
<meta property="og:description" content="A data science blog" />
<meta property="og:type" content="page" />
<meta property="og:url" content="https://juliasilge.com/blog/gpt-detectors/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://juliasilge.com/blog/gpt-detectors/featured.png" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://juliasilge.com/blog/gpt-detectors/featured.png" >
    
    
  <meta itemprop="name" content="Classification metrics for #TidyTuesday GPT detectors">
<meta itemprop="description" content="Learn about different kinds of metrics for evaluating classification models, and how to compute, compare, and visualize them."><meta itemprop="datePublished" content="2023-07-19T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-07-19T00:00:00+00:00" />
<meta itemprop="wordCount" content="1741"><meta itemprop="image" content="https://juliasilge.com/blog/gpt-detectors/featured.png">
<meta itemprop="keywords" content="rstats,tidymodels," />
  
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/icon.png" type="image/x-icon">
  <link rel="icon" href="/img/icon.png" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.34750661ca065ca7ddb87b2b28cab82abf493a21c1e3852e0755fba776b031b7.css" integrity="sha256-NHUGYcoGXKfduHsrKMq4Kr9JOiHB44UuB1X7p3awMbc=" media="screen">
  
  
  <script src="/panelset.min.078a92db9bd3228df502db3d9e0453c3cf3d910abe3f8deca0ad196c7071ad41.js" type="text/javascript"></script>
  
  
  <script src="/main.min.bb67dea4a2ee41aab688effd180f2d02662e47280f0021495f2c0ce24c461f65.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://juliasilge.com/" title="Home">
      <span class="f4 fw7">Julia Silge</span>
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About me">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Classification metrics for #TidyTuesday GPT detectors</h1>
        
        <p class="f6 measure lh-copy mv1">By Julia Silge in <a href="https://juliasilge.com/categories/rstats">rstats</a>  <a href="https://juliasilge.com/categories/tidymodels">tidymodels</a> </p>
        <p class="f7 db mv0 ttu">July 19, 2023</p>

      

      </header>
      <section class="post-body pt5 pb4">
        <p>This is the latest in my series of 
<a href="https://www.youtube.com/juliasilge" target="_blank" rel="noopener">screencasts</a>! This screencast focuses on how to use tidymodels for computing classification metrics, using this week‚Äôs 
<a href="https://github.com/rfordatascience/tidytuesday" target="_blank" rel="noopener"><code>#TidyTuesday</code> dataset</a> on GPT detectors. ü§ñ</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube-nocookie.com/embed/8N5zIHSzJoE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>
</br>
<p>Here is the code I used in the video, for those who prefer reading instead of or in addition to video.</p>




<h2 id="explore-data">Explore data
  <a href="#explore-data"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>We‚Äôre not going to train a model here, but instead use the output (i.e.¬†predictions) from a handful of models that have been trained to 
<a href="https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-07-18/readme.md" target="_blank" rel="noopener">detect text output from GPT</a>. Let‚Äôs start by reading in the data:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#06287e">library</span>(tidyverse)
<span style="color:#06287e">library</span>(detectors)

detectors
</code></pre></div><pre><code>## # A tibble: 6,185 √ó 9
##    kind  .pred_AI .pred_class detector     native name  model document_id prompt
##    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;       &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; 
##  1 Human 1.00     AI          Sapling      No     Real‚Ä¶ Human         497 &lt;NA&gt;  
##  2 Human 0.828    AI          Crossplag    No     Real‚Ä¶ Human         278 &lt;NA&gt;  
##  3 Human 0.000214 Human       Crossplag    Yes    Real‚Ä¶ Human         294 &lt;NA&gt;  
##  4 AI    0        Human       ZeroGPT      &lt;NA&gt;   Fake‚Ä¶ GPT3          671 Plain 
##  5 AI    0.00178  Human       Originality‚Ä¶ &lt;NA&gt;   Fake‚Ä¶ GPT4          717 Eleva‚Ä¶
##  6 Human 0.000178 Human       HFOpenAI     Yes    Real‚Ä¶ Human         855 &lt;NA&gt;  
##  7 AI    0.992    AI          HFOpenAI     &lt;NA&gt;   Fake‚Ä¶ GPT3          533 Plain 
##  8 AI    0.0226   Human       Crossplag    &lt;NA&gt;   Fake‚Ä¶ GPT4          484 Eleva‚Ä¶
##  9 Human 0        Human       ZeroGPT      Yes    Real‚Ä¶ Human         781 &lt;NA&gt;  
## 10 Human 1.00     AI          Sapling      No     Real‚Ä¶ Human         460 &lt;NA&gt;  
## # ‚Ñπ 6,175 more rows
</code></pre>
<p>The <code>kind</code> variable tells us whether a document was written by a human or generated via GPT, and the two <code>.pred_*</code> variables tells us what <code>detector</code> thought about that text, the predicted probability (<code>.pred_AI</code>) and predicted class (<code>.pred_class</code>) of that text being generated by AI. The <code>native</code> variable records whether a certain document was written by a native English writer or not.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">count</span>(native, kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 6 √ó 4
##   native kind  .pred_class     n
##   &lt;chr&gt;  &lt;fct&gt; &lt;fct&gt;       &lt;int&gt;
## 1 No     Human AI            390
## 2 No     Human Human         247
## 3 Yes    Human AI             59
## 4 Yes    Human Human        1772
## 5 &lt;NA&gt;   AI    AI           1158
## 6 &lt;NA&gt;   AI    Human        2559
</code></pre>
<p>This is a great example for talking about classification metrics, because we are in a pretty common situation where a variable of interest (<code>native</code>) only applies to one class, the real humans. The AI-generated texts were certainly not generated by a native English writer, but they also were not generated by a non-native English writer.</p>
<p>For the real humans, what is the distribution of predicted probability for native and non-native English writers?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">filter</span>(<span style="color:#666">!</span><span style="color:#06287e">is.na</span>(native)) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">mutate</span>(native <span style="color:#666">=</span> <span style="color:#06287e">case_when</span>(native <span style="color:#666">==</span> <span style="color:#4070a0">&#34;Yes&#34;</span> <span style="color:#666">~</span> <span style="color:#4070a0">&#34;Native English writer&#34;</span>,
                            native <span style="color:#666">==</span> <span style="color:#4070a0">&#34;No&#34;</span> <span style="color:#666">~</span> <span style="color:#4070a0">&#34;Non-native English writer&#34;</span>)) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">ggplot</span>(<span style="color:#06287e">aes</span>(.pred_AI, fill <span style="color:#666">=</span> native)) <span style="color:#666">+</span>
  <span style="color:#06287e">geom_histogram</span>(bins <span style="color:#666">=</span> <span style="color:#40a070">40</span>, show.legend <span style="color:#666">=</span> <span style="color:#007020;font-weight:bold">FALSE</span>) <span style="color:#666">+</span>
  <span style="color:#06287e">facet_wrap</span>(<span style="color:#06287e">vars</span>(native), scales <span style="color:#666">=</span> <span style="color:#4070a0">&#34;free_y&#34;</span>, nrow <span style="color:#666">=</span> <span style="color:#40a070">2</span>) 
</code></pre></div><img src="https://juliasilge.com/blog/gpt-detectors/index_files/figure-html/unnamed-chunk-4-1.png" width="1260" />
<p>This is the main point of the 
<a href="https://arxiv.org/abs/2304.02819" target="_blank" rel="noopener">paper this data comes from</a>: GPT detectors are much more wrong for non-native English writers than for native writers. Pretty dramatic! Let‚Äôs walk through how to use classification metrics to measure this in a different way from looking at the distribution overall.</p>
<p>These GPT detectors are classification models (predicting ‚Äúhuman‚Äù vs.¬†‚ÄúAI‚Äù), and there are two main categories of metrics for classification models:</p>
<ul>
<li>Metrics that use a predicted <strong>class</strong>, like ‚Äúhuman‚Äù or ‚ÄúAI‚Äù</li>
<li>Metrics that use a predicted <strong>probability</strong>, like <code>.pred_AI = 0.828</code></li>
</ul>




<h2 id="metrics-that-use-a-predicted-class">Metrics that use a predicted class
  <a href="#metrics-that-use-a-predicted-class"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>In 
<a href="https://www.tidymodels.org/" target="_blank" rel="noopener">tidymodels</a>, the package that handles model metrics is 
<a href="https://yardstick.tidymodels.org/" target="_blank" rel="noopener">yardstick</a>. Let‚Äôs start by making a confusion matrix, which uses the predicted classes, for the dataset as a whole:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#06287e">library</span>(yardstick)

detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">conf_mat</span>(kind, .pred_class)
</code></pre></div><pre><code>##           Truth
## Prediction   AI Human
##      AI    1158   449
##      Human 2559  2019
</code></pre>
<p>This <code>\(2 \times 2\)</code> matrix or table tells us specifics about how these models are right and wrong. Overall, these models look to be better at identifying human documents (2019 right vs.¬†449 wrong) than identifying AI documents (1158 right vs.¬†2559 wrong).</p>
<p>One of the most common metrics for classification models is accuracy, just the plain old proportion of our data that is predicted correctly:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">accuracy</span>(kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 1 √ó 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.514
</code></pre>
<p>Random guessing for a binary classification problem will give you 0.5, so here we see that these detectors are not much better than random guessing, aggregated over all these documents and different models/detectors. Do some detectors do better than others?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">group_by</span>(detector) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">accuracy</span>(kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 7 √ó 4
##   detector      .metric  .estimator .estimate
##   &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 Crossplag     accuracy binary         0.501
## 2 GPTZero       accuracy binary         0.489
## 3 HFOpenAI      accuracy binary         0.514
## 4 OriginalityAI accuracy binary         0.590
## 5 Quil          accuracy binary         0.478
## 6 Sapling       accuracy binary         0.5  
## 7 ZeroGPT       accuracy binary         0.517
</code></pre>
<p>There is a little bit of variation here, I guess. What about differences across the <code>native</code> variable?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">group_by</span>(native) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">accuracy</span>(kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 3 √ó 4
##   native .metric  .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 No     accuracy binary         0.388
## 2 Yes    accuracy binary         0.968
## 3 &lt;NA&gt;   accuracy binary         0.312
</code></pre>
<p>This is now a much bigger difference. Notice that the accuracy for native English writers is <strong>GREAT</strong> while for non-native English writers it‚Äôs not much better than for the AI documents (remember that <code>native = NA</code> here means an AI generated document).</p>
<p>If you‚Äôve learned a bit about ML, you probably have heard that accuracy is often not a great metric (especially when you have much class imbalance) so let‚Äôs try out a pair of related metrics that tell us 
<a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" target="_blank" rel="noopener">the true positive rate and true negative rate</a>.</p>
<p>Sensitivity tells us the true positive rate:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">sensitivity</span>(kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 1 √ó 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 sensitivity binary         0.312
</code></pre>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">group_by</span>(detector) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">sensitivity</span>(kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 7 √ó 4
##   detector      .metric     .estimator .estimate
##   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 Crossplag     sensitivity binary         0.249
## 2 GPTZero       sensitivity binary         0.202
## 3 HFOpenAI      sensitivity binary         0.271
## 4 OriginalityAI sensitivity binary         0.444
## 5 Quil          sensitivity binary         0.379
## 6 Sapling       sensitivity binary         0.392
## 7 ZeroGPT       sensitivity binary         0.245
</code></pre>
<p>Sensitivity is a metric that can vary between 0 (bad) and 1 (good) so this looks like we will have lots of false negatives. Can we measure the true positive rate for different types of human writers?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">group_by</span>(native) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">sensitivity</span>(kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 3 √ó 4
##   native .metric     .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 No     sensitivity binary        NA    
## 2 Yes    sensitivity binary        NA    
## 3 &lt;NA&gt;   sensitivity binary         0.312
</code></pre>
<p>No, we can‚Äôt! Since there are no AI-generated documents written by native (or non-native) English writers, we can‚Äôt compute any metrics that need those empty elements of the confusion matrix:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">filter</span>(<span style="color:#666">!</span><span style="color:#06287e">is.na</span>(native)) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">conf_mat</span>(kind, .pred_class)
</code></pre></div><pre><code>##           Truth
## Prediction   AI Human
##      AI       0   449
##      Human    0  2019
</code></pre>
<p>Specificity, the true negative rate, works the same way, but ‚Äúflipped‚Äù:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">group_by</span>(native) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">specificity</span>(kind, .pred_class)
</code></pre></div><pre><code>## # A tibble: 3 √ó 4
##   native .metric     .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 No     specificity binary         0.388
## 2 Yes    specificity binary         0.968
## 3 &lt;NA&gt;   specificity binary        NA
</code></pre>
<p>We can see that the true negative rate is much better for native English writers than non-native ones, but we can‚Äôt compute a true negative rate for <em>only</em> the AI-generated documents. This kind of situation comes up a fair amount for real models where a variable you are interested in only applies to one class in your classification problem.</p>




<h2 id="metrics-that-use-a-predicted-probability">Metrics that use a predicted probability
  <a href="#metrics-that-use-a-predicted-probability"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>The other main category of metrics for this type of model use a probability rather than a class. If you have ever seen or used an ROC curve, it is an example of model evaluation that uses probabilities:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">roc_curve</span>(kind, .pred_AI) <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">autoplot</span>()
</code></pre></div><img src="https://juliasilge.com/blog/gpt-detectors/index_files/figure-html/unnamed-chunk-13-1.png" width="1260" />
<p>Notice how we‚Äôre using <code>.pred_AI</code> instead of <code>.pred_class</code>. How do the different detectors/models do in an ROC curve?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">group_by</span>(detector) <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">roc_curve</span>(kind, .pred_AI) <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">autoplot</span>()
</code></pre></div><img src="https://juliasilge.com/blog/gpt-detectors/index_files/figure-html/unnamed-chunk-14-1.png" width="1260" />
<p>Yikes, OK. These curves can be used to compute the ROC AUC (area under the ROC curve):</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">group_by</span>(detector) <span style="color:#666">|&gt;</span> 
  <span style="color:#06287e">roc_auc</span>(kind, .pred_AI) 
</code></pre></div><pre><code>## # A tibble: 7 √ó 4
##   detector      .metric .estimator .estimate
##   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 Crossplag     roc_auc binary         0.613
## 2 GPTZero       roc_auc binary         0.750
## 3 HFOpenAI      roc_auc binary         0.614
## 4 OriginalityAI roc_auc binary         0.682
## 5 Quil          roc_auc binary         0.584
## 6 Sapling       roc_auc binary         0.480
## 7 ZeroGPT       roc_auc binary         0.603
</code></pre>
<p>Can we compute this kind of metric and compare across the <code>native</code> categories?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">group_by</span>(native) <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">roc_curve</span>(kind, .pred_AI)
</code></pre></div><pre><code>## Error in `roc_curve()`:
## ! No event observations were detected in `truth` with event level 'AI'.
</code></pre>
<p>Unfortunately, no, because in, say, the non-native English writer category, there are no AI-generated texts.</p>
<p>Let‚Äôs think about a metric that works in a different way, like 
<a href="https://yardstick.tidymodels.org/reference/mn_log_loss.html" target="_blank" rel="noopener">mean log loss</a>. This metric can distinguish between predictions that are a little wrong vs.¬†very wrong; for example, it will say that a real human document with <code>.pred_AI = 0.8</code> is predicted worse than one with <code>.pred_AI = 0.6</code>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">mn_log_loss</span>(kind, .pred_AI)
</code></pre></div><pre><code>## # A tibble: 1 √ó 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 mn_log_loss binary          4.73
</code></pre>
<p>A log loss is better when it is lower. How does log loss vary over the detectors/models?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">group_by</span>(detector) <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">mn_log_loss</span>(kind, .pred_AI) <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">arrange</span>(.estimate)
</code></pre></div><pre><code>## # A tibble: 7 √ó 4
##   detector      .metric     .estimator .estimate
##   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 OriginalityAI mn_log_loss binary          1.94
## 2 Crossplag     mn_log_loss binary          2.81
## 3 HFOpenAI      mn_log_loss binary          2.83
## 4 Quil          mn_log_loss binary          3.18
## 5 GPTZero       mn_log_loss binary          4.60
## 6 Sapling       mn_log_loss binary          5.82
## 7 ZeroGPT       mn_log_loss binary         11.9
</code></pre>
<p>This is a pretty dramatic range in log loss. üò≥ What about across the <code>native</code> variable?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">detectors <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">group_by</span>(native) <span style="color:#666">|&gt;</span> 
    <span style="color:#06287e">mn_log_loss</span>(kind, .pred_AI)
</code></pre></div><pre><code>## # A tibble: 3 √ó 4
##   native .metric     .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 No     mn_log_loss binary         3.60 
## 2 Yes    mn_log_loss binary         0.116
## 3 &lt;NA&gt;   mn_log_loss binary         7.21
</code></pre>
<p>Here we see that the documents written by non-native English writers are predicted better than the AI-generated documents, but way, way worse than those by native Engish writers.</p>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">July 19, 2023</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">9 minute read, 1741 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="https://juliasilge.com/categories/rstats">rstats</a>  <a href="https://juliasilge.com/categories/tidymodels">tidymodels</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Tags:</dt>
    <dd class="fw5 ml0"> <a href="https://juliasilge.com/tags/rstats">rstats</a>  <a href="https://juliasilge.com/tags/tidymodels">tidymodels</a> </dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
    <dd class="fw5 ml0"><a href="/blog/educational-attainment/">Educational attainment in #TidyTuesday UK towns</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/polling-places/">Changes in #TidyTuesday US polling places</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/doctor-who-bayes/">Empirical Bayes for #TidyTuesday Doctor Who episodes</a></dd>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="https://juliasilge.com/blog/spam-email/">&larr; Evaluate multiple modeling approaches for #TidyTuesday spam email</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="https://juliasilge.com/blog/place-names/">What tokens are used more vs. less in #TidyTuesday place names? &rarr;</a>
  
</div>

      </footer>
    </article>
    
      
<div class="post-comments pa0 pa4-l mt4">
  
  <script src="https://utteranc.es/client.js"
          repo="juliasilge/juliasilge.com"
          issue-term="title"
          theme="github-light"
          label="comments :speech_balloon:"
          crossorigin="anonymous"
          async
          type="text/javascript">
  </script>
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2024 Julia Silge
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Ap√©ro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
